{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a05064dc",
   "metadata": {},
   "source": [
    "# IMDB Review Sentiment Analysis\n",
    "\n",
    "\n",
    "Data source: https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format\n",
    "\n",
    "Code Reference: https://www.kaggle.com/code/miladlink/imdb-sentiment-analysis-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3a59be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# setup\n",
    "NLP = spacy.load('en_core_web_sm')  # NLP toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021b4193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train: 40000\n",
      "number of valid: 5000\n",
      "number of test: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0\n",
       "1  When I put this movie in my DVD player, and sa...      0\n",
       "2  Why do people who do not know what a particula...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset\n",
    "train_df = pd.read_csv(\"Train.csv\")\n",
    "valid_df = pd.read_csv(\"Valid.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c74309",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de27857",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of train:', len(train_df))\n",
    "print('number of valid:', len(valid_df))\n",
    "print('number of test:', len(test_df))\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b404911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suenchihang/opt/anaconda3/envs/rosetta_seq/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAV/0lEQVR4nO3df6xf9X3f8eerOKFkiRk/bjJqm5omTjRgmTNfeaxRomRsw426mETQGW3BW5GcMLI1ajUtdNISdfIU1qSoZMWVM6hxmvBjEIYnhbaMVImyEuglpZgfYbkEGm5sgVNQ4i6FxeS9P76fb/L19dc3Fx9/v19u7vMhHd3zfZ/zOd/PQZZefM7nfM9JVSFJ0rH6qUl3QJK0tBkkkqRODBJJUicGiSSpE4NEktTJikl3YNxOP/30Wrt27aS7IUlLyv333//tqpoatm3ZBcnatWuZmZmZdDckaUlJ8hdH2+alLUlSJwaJJKkTg0SS1IlBIknqxCCRJHVikEiSOhlZkCRZk+SPkzya5OEkv9Lqpya5K8nX299TBtpcmWQ2yWNJLhiob0iyt227Jkla/cQkN7f6vUnWjup8JEnDjXJEcgj4tar628B5wBVJzgY+DNxdVeuAu9tn2rYtwDnAJuDaJCe0Y+0AtgHr2rKp1S8DnquqNwBXA1eN8HwkSUOMLEiqan9VfbWtHwQeBVYBm4Eb2m43ABe29c3ATVX1QlU9AcwCG5OcAaysqnuq9/KU3fPa9I91K3B+f7QiSRqPsfyyvV1yegtwL/C6qtoPvbBJ8tq22yrgKwPN5lrt+219fr3f5ql2rENJvgOcBnx73vdvozei4cwzz+x8Phv+3e7Ox9BPnvt/89JJd4Fv/sbfmXQX9DJ05n/cO9Ljj3yyPcmrgduAD1XVdxfadUitFqgv1ObwQtXOqpququmpqaGPipEkHaORBkmSV9ALkc9U1eda+el2uYr295lWnwPWDDRfDexr9dVD6oe1SbICOBl49vifiSTpaEZ511aA64BHq+q3BjbtAba29a3AHQP1Le1OrLPoTarf1y6DHUxyXjvmpfPa9I91EfCF8iX0kjRWo5wjeSvwPmBvkgda7deBjwG3JLkM+CZwMUBVPZzkFuARend8XVFVL7Z2lwO7gJOAO9sCvaD6dJJZeiORLSM8H0nSECMLkqr6MsPnMADOP0qb7cD2IfUZ4Nwh9edpQSRJmgx/2S5J6sQgkSR1YpBIkjoxSCRJnRgkkqRODBJJUicGiSSpE4NEktSJQSJJ6sQgkSR1YpBIkjoxSCRJnRgkkqRODBJJUicGiSSpE4NEktSJQSJJ6mSU72y/PskzSR4aqN2c5IG2PNl/BW+StUn+emDb7w602ZBkb5LZJNe097bT3u1+c6vfm2TtqM5FknR0oxyR7AI2DRaq6p9V1fqqWg/cBnxuYPPj/W1V9YGB+g5gG7CuLf1jXgY8V1VvAK4GrhrJWUiSFjSyIKmqLwHPDtvWRhW/BNy40DGSnAGsrKp7qqqA3cCFbfNm4Ia2fitwfn+0Ikkan0nNkbwNeLqqvj5QOyvJnyX5YpK3tdoqYG5gn7lW6297CqCqDgHfAU4b9mVJtiWZSTJz4MCB43kekrTsTSpILuHw0ch+4Myqegvwq8Bnk6wEho0wqv1daNvhxaqdVTVdVdNTU1Mdui1Jmm/FuL8wyQrgvcCGfq2qXgBeaOv3J3kceCO9EcjqgeargX1tfQ5YA8y1Y57MUS6lSZJGZxIjkn8EfK2qfnjJKslUkhPa+s/Rm1T/RlXtBw4mOa/Nf1wK3NGa7QG2tvWLgC+0eRRJ0hiN8vbfG4F7gDclmUtyWdu0hSMn2d8OPJjkz+lNnH+gqvqji8uB/wbMAo8Dd7b6dcBpSWbpXQ778KjORZJ0dCO7tFVVlxyl/i+H1G6jdzvwsP1ngHOH1J8HLu7WS0lSV/6yXZLUiUEiSerEIJEkdWKQSJI6MUgkSZ0YJJKkTgwSSVInBokkqRODRJLUiUEiSerEIJEkdWKQSJI6MUgkSZ0YJJKkTgwSSVInBokkqZNRviHx+iTPJHlooPbRJN9K8kBb3jWw7coks0keS3LBQH1Dkr1t2zXtlbskOTHJza1+b5K1ozoXSdLRjXJEsgvYNKR+dVWtb8vnAZKcTe8VvOe0Ntf23+EO7AC20XuP+7qBY14GPFdVbwCuBq4a1YlIko5uZEFSVV8Cnv2xO/ZsBm6qqheq6gl672ffmOQMYGVV3VNVBewGLhxoc0NbvxU4vz9akSSNzyTmSD6Y5MF26euUVlsFPDWwz1yrrWrr8+uHtamqQ8B3gNNG2XFJ0pHGHSQ7gNcD64H9wCdafdhIohaoL9TmCEm2JZlJMnPgwIGX1GFJ0sLGGiRV9XRVvVhVPwA+BWxsm+aANQO7rgb2tfrqIfXD2iRZAZzMUS6lVdXOqpququmpqanjdTqSJMYcJG3Oo+89QP+Orj3AlnYn1ln0JtXvq6r9wMEk57X5j0uBOwbabG3rFwFfaPMokqQxWjGqAye5EXgHcHqSOeAjwDuSrKd3CepJ4P0AVfVwkluAR4BDwBVV9WI71OX07gA7CbizLQDXAZ9OMktvJLJlVOciSTq6kQVJVV0ypHzdAvtvB7YPqc8A5w6pPw9c3KWPkqTu/GW7JKkTg0SS1IlBIknqxCCRJHVikEiSOjFIJEmdGCSSpE4MEklSJwaJJKkTg0SS1IlBIknqxCCRJHVikEiSOjFIJEmdGCSSpE4MEklSJwaJJKmTkQVJkuuTPJPkoYHabyb5WpIHk9ye5G+2+tokf53kgbb87kCbDUn2JplNck17dzvt/e43t/q9SdaO6lwkSUc3yhHJLmDTvNpdwLlV9Wbg/wBXDmx7vKrWt+UDA/UdwDZgXVv6x7wMeK6q3gBcDVx1/E9BkvTjjCxIqupLwLPzan9UVYfax68Aqxc6RpIzgJVVdU9VFbAbuLBt3gzc0NZvBc7vj1YkSeMzyTmSXwbuHPh8VpI/S/LFJG9rtVXA3MA+c63W3/YUQAun7wCnDfuiJNuSzCSZOXDgwPE8B0la9iYSJEn+A3AI+Ewr7QfOrKq3AL8KfDbJSmDYCKP6h1lg2+HFqp1VNV1V01NTU906L0k6zIpxf2GSrcAvAue3y1VU1QvAC239/iSPA2+kNwIZvPy1GtjX1ueANcBckhXAycy7lCZJGr2xjkiSbAL+PfDuqvreQH0qyQlt/efoTap/o6r2AweTnNfmPy4F7mjN9gBb2/pFwBf6wSRJGp+RjUiS3Ai8Azg9yRzwEXp3aZ0I3NXmxb/S7tB6O/AbSQ4BLwIfqKr+6OJyeneAnURvTqU/r3Id8Okks/RGIltGdS6SpKMbWZBU1SVDytcdZd/bgNuOsm0GOHdI/Xng4i59lCR15y/bJUmdGCSSpE4MEklSJwaJJKmTRQVJkrsXU5MkLT8L3rWV5KeBV9G7hfcUfvRr8pXAz4y4b5KkJeDH3f77fuBD9ELjfn4UJN8Ffmd03ZIkLRULBklV/Tbw20n+TVV9ckx9kiQtIYv6QWJVfTLJzwNrB9tU1e4R9UuStEQsKkiSfBp4PfAAvUeYQO9JuwaJJC1zi31EyjRwtg9FlCTNt9jfkTwE/K1RdkSStDQtdkRyOvBIkvto7w0BqKp3j6RXkqQlY7FB8tFRdkKStHQt9q6tL466I5KkpWmxd20d5EfvQ38l8Arg/1bVylF1TJK0NCx2RPKawc9JLgQ2jqJDkqSl5Zie/ltV/wP4hwvtk+T6JM8keWigdmqSu5J8vf09ZWDblUlmkzyW5IKB+oYke9u2a9q720lyYpKbW/3eJGuP5VwkSd0s9um/7x1YLkryMX50qetodgGb5tU+DNxdVeuAu9tnkpxN753r57Q21yY5obXZAWwD1rWlf8zLgOeq6g3A1cBVizkXSdLxtdgRyT8dWC4ADgKbF2pQVV8Cnp1X3gzc0NZvAC4cqN9UVS9U1RPALLAxyRnAyqq6p/0Ycve8Nv1j3Qqc3x+tSJLGZ7FzJP/qOH3f66pqfzvm/iSvbfVVwFcG9ptrte+39fn1fpun2rEOJfkOcBrw7flfmmQbvVENZ5555nE6FUkSLP7S1uokt7c5j6eT3JZk9XHsx7CRRC1QX6jNkcWqnVU1XVXTU1NTx9hFSdIwi7209XvAHnrvJVkF/M9We6mebperaH+fafU5YM3AfquBfa2+ekj9sDZJVgAnc+SlNEnSiC02SKaq6veq6lBbdgHH8r/2e4CtbX0rcMdAfUu7E+ssepPq97XLYAeTnNfmPy6d16Z/rIuAL/hQSUkav8U+IuXbSf4FcGP7fAnwlws1SHIj8A56r+mdAz4CfAy4JcllwDeBiwGq6uEktwCPAIeAK6qq/7j6y+ndAXYScGdbAK4DPp1klt5IZMsiz0WSdBwtNkh+Gfiv9G6zLeBPgAUn4KvqkqNsOv8o+28Htg+pzwDnDqk/TwsiSdLkLDZI/hOwtaqeg94PC4GP0wsYSdIyttg5kjf3QwSgqp4F3jKaLkmSlpLFBslPzXucyaksfjQjSfoJttgw+ATwJ0lupTdH8ksMmc+QJC0/i/1l++4kM/Qe1BjgvVX1yEh7JklaEhZ9eaoFh+EhSTrMMT1GXpKkPoNEktSJQSJJ6sQgkSR1YpBIkjoxSCRJnRgkkqRODBJJUicGiSSpE4NEktSJQSJJ6mTsQZLkTUkeGFi+m+RDST6a5FsD9XcNtLkyyWySx5JcMFDfkGRv23ZNe6+7JGmMxh4kVfVYVa2vqvXABuB7wO1t89X9bVX1eYAkZ9N7H/s5wCbg2iQntP13ANuAdW3ZNL4zkSTB5C9tnQ88XlV/scA+m4GbquqFqnoCmAU2JjkDWFlV91RVAbuBC0feY0nSYSYdJFuAGwc+fzDJg0muH3gj4yrgqYF95lptVVufXz9Ckm1JZpLMHDhw4Pj1XpI0uSBJ8krg3cB/b6UdwOuB9cB+em9lhN6LtOarBepHFqt2VtV0VU1PTU116bYkaZ5Jjkh+AfhqVT0NUFVPV9WLVfUD4FPAxrbfHLBmoN1qYF+rrx5SlySN0SSD5BIGLmu1OY++9wAPtfU9wJYkJyY5i96k+n1VtR84mOS8drfWpcAd4+m6JKlv0a/aPZ6SvAr4x8D7B8r/Jcl6epennuxvq6qHk9xC7zW/h4ArqurF1uZyYBdwEnBnWyRJYzSRIKmq7wGnzau9b4H9twPbh9RngHOPewclSYs26bu2JElLnEEiSerEIJEkdWKQSJI6MUgkSZ0YJJKkTgwSSVInBokkqRODRJLUiUEiSerEIJEkdWKQSJI6MUgkSZ0YJJKkTgwSSVInBokkqRODRJLUyUSCJMmTSfYmeSDJTKudmuSuJF9vf08Z2P/KJLNJHktywUB9QzvObJJr2rvbJUljNMkRyTuran1VTbfPHwburqp1wN3tM0nOBrYA5wCbgGuTnNDa7AC2AevasmmM/Zck8fK6tLUZuKGt3wBcOFC/qapeqKongFlgY5IzgJVVdU9VFbB7oI0kaUwmFSQF/FGS+5Nsa7XXVdV+gPb3ta2+CnhqoO1cq61q6/PrR0iyLclMkpkDBw4cx9OQJK2Y0Pe+tar2JXktcFeSry2w77B5j1qgfmSxaiewE2B6enroPpKkYzOREUlV7Wt/nwFuBzYCT7fLVbS/z7Td54A1A81XA/taffWQuiRpjMYeJEn+RpLX9NeBfwI8BOwBtrbdtgJ3tPU9wJYkJyY5i96k+n3t8tfBJOe1u7UuHWgjSRqTSVzaeh1we7tTdwXw2ar6gyR/CtyS5DLgm8DFAFX1cJJbgEeAQ8AVVfViO9blwC7gJODOtkiSxmjsQVJV3wD+7pD6XwLnH6XNdmD7kPoMcO7x7qMkafFeTrf/SpKWIINEktSJQSJJ6sQgkSR1YpBIkjoxSCRJnRgkkqRODBJJUicGiSSpE4NEktSJQSJJ6sQgkSR1YpBIkjoxSCRJnRgkkqRODBJJUicGiSSpk0m8s31Nkj9O8miSh5P8Sqt/NMm3kjzQlncNtLkyyWySx5JcMFDfkGRv23ZNe3e7JGmMJvHO9kPAr1XVV5O8Brg/yV1t29VV9fHBnZOcDWwBzgF+BvhfSd7Y3tu+A9gGfAX4PLAJ39suSWM19hFJVe2vqq+29YPAo8CqBZpsBm6qqheq6glgFtiY5AxgZVXdU1UF7AYuHG3vJUnzTXSOJMla4C3Ava30wSQPJrk+ySmttgp4aqDZXKutauvz68O+Z1uSmSQzBw4cOJ6nIEnL3sSCJMmrgduAD1XVd+ldpno9sB7YD3yiv+uQ5rVA/chi1c6qmq6q6ampqa5dlyQNmEiQJHkFvRD5TFV9DqCqnq6qF6vqB8CngI1t9zlgzUDz1cC+Vl89pC5JGqNJ3LUV4Drg0ar6rYH6GQO7vQd4qK3vAbYkOTHJWcA64L6q2g8cTHJeO+alwB1jOQlJ0g9N4q6ttwLvA/YmeaDVfh24JMl6epenngTeD1BVDye5BXiE3h1fV7Q7tgAuB3YBJ9G7W8s7tiRpzMYeJFX1ZYbPb3x+gTbbge1D6jPAucevd5Kkl8pftkuSOjFIJEmdGCSSpE4MEklSJwaJJKkTg0SS1IlBIknqxCCRJHVikEiSOjFIJEmdGCSSpE4MEklSJwaJJKkTg0SS1IlBIknqxCCRJHVikEiSOlnyQZJkU5LHkswm+fCk+yNJy82SDpIkJwC/A/wCcDa9976fPdleSdLysqSDBNgIzFbVN6rq/wE3AZsn3CdJWlZWTLoDHa0Cnhr4PAf8/fk7JdkGbGsf/yrJY2Po23JxOvDtSXfi5SAf3zrpLuhw/tvs+0iOx1F+9mgblnqQDPuvU0cUqnYCO0ffneUnyUxVTU+6H9J8/tscn6V+aWsOWDPweTWwb0J9kaRlaakHyZ8C65KcleSVwBZgz4T7JEnLypK+tFVVh5J8EPhD4ATg+qp6eMLdWm68ZKiXK/9tjkmqjphSkCRp0Zb6pS1J0oQZJJKkTgwSHRMfTaOXqyTXJ3kmyUOT7styYZDoJfPRNHqZ2wVsmnQnlhODRMfCR9PoZauqvgQ8O+l+LCcGiY7FsEfTrJpQXyRNmEGiY7GoR9NIWh4MEh0LH00j6YcMEh0LH00j6YcMEr1kVXUI6D+a5lHgFh9No5eLJDcC9wBvSjKX5LJJ9+knnY9IkSR14ohEktSJQSJJ6sQgkSR1YpBIkjoxSCRJnRgk0ggl+asfs33tS31KbZJdSS7q1jPp+DFIJEmdGCTSGCR5dZK7k3w1yd4kg09LXpHkhiQPJrk1yatamw1Jvpjk/iR/mOSMCXVfWpBBIo3H88B7qurvAe8EPpGk//DLNwE7q+rNwHeBf53kFcAngYuqagNwPbB9Av2WfqwVk+6AtEwE+M9J3g78gN5j91/Xtj1VVf+7rf8+8G+BPwDOBe5qeXMCsH+sPZYWySCRxuOfA1PAhqr6fpIngZ9u2+Y/p6joBc/DVfUPxtdF6dh4aUsaj5OBZ1qIvBP42YFtZybpB8YlwJeBx4Cpfj3JK5KcM9YeS4tkkEjj8RlgOskMvdHJ1wa2PQpsTfIgcCqwo73C+CLgqiR/DjwA/Px4uywtjk//lSR14ohEktSJQSJJ6sQgkSR1YpBIkjoxSCRJnRgkkqRODBJJUif/H/RFTVxqzB9EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(valid_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f52f6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cce62004b044c8a97334a532b7585e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length = 4\n",
      "Max length = 2470\n",
      "Mean = 231.33\n",
      "Std  = 171.18\n",
      "mean + 2 * sigma = 573.68\n"
     ]
    }
   ],
   "source": [
    "num_words = [len(f.split(' ')) for f in tqdm.notebook.tqdm(train_df['text'])]\n",
    "\n",
    "# print statistics\n",
    "print('Min length =', min(num_words))\n",
    "print('Max length =', max(num_words))\n",
    "\n",
    "print('Mean = {:.2f}'.format(np.mean(num_words)))\n",
    "print('Std  = {:.2f}'.format(np.std(num_words)))\n",
    "\n",
    "print('mean + 2 * sigma = {:.2f}'.format(np.mean(num_words) + 2.0 * np.std(num_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827e209",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2223f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub(r\"<br /><br />\", \" \", text)\n",
    "    text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    return [x.text for x in NLP.tokenizer(text) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa98d72",
   "metadata": {},
   "source": [
    "## Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17529ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.count = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2index:\n",
    "            self.word2index[word] = self.count\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.count] = word\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in self.tokenizer(sentence):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0f994a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'  # special symbol we use for padding text\n",
    "UNK = '<unk>'  # special symbol we use for rare or unknown word\n",
    "CLASSES = ['negative', 'positive']\n",
    "MAX_LEN = 400\n",
    "MIN_COUNT = 10\n",
    "VOCAB_PATH = 'vocab.pkl'\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ba1fd",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f523aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, tokenizer, classes, vocab_path='vocab.pkl', max_len=100, min_count=10):\n",
    "        \n",
    "        self.df = df\n",
    "        self.vocab_path = vocab_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.vocab = None\n",
    "        \n",
    "        self.classes = classes\n",
    "        self.class_idx = np.unique(df['label'])\n",
    "        self.class_to_index = {classes[i]: i for i in range(len(classes))}        \n",
    "        self.num_classes = len(self.classes)\n",
    "        self.text = df['text']\n",
    "            \n",
    "        # build vocabulary from training and validation texts\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):        \n",
    "        # read text file \n",
    "        text, label = self.df['text'][index], self.df['label'][index]\n",
    "        \n",
    "        # tokenize the text file\n",
    "        tokens = self.tokenizer(text.lower().strip())\n",
    "        \n",
    "        # padding and trimming\n",
    "        if len(tokens) < self.max_len:\n",
    "            num_pads = self.max_len - len(tokens)\n",
    "            tokens = [PAD] * num_pads + tokens\n",
    "        elif len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        # numericalizing\n",
    "        ids = torch.LongTensor(self.max_len)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in self.vocab.word2index:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # unknown words\n",
    "            elif word != PAD and self.vocab.word2count[word] < self.min_count:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # rare words\n",
    "            else:\n",
    "                ids[i] = self.vocab.word2index[word]\n",
    "        \n",
    "        return ids, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if not os.path.exists(self.vocab_path):\n",
    "            vocab = Vocabulary(self.tokenizer)\n",
    "            for f in tqdm.notebook.tqdm(self.df['text'], desc='Building Vocab'):\n",
    "                for line in f.split('\\n'):\n",
    "                    vocab.add_sentence(line.lower())\n",
    "            # sort words by their frequencies--------\n",
    "            words = [(0, PAD), (0, UNK)]\n",
    "            words += sorted([(c, w) for w, c in vocab.word2count.items()], reverse=True)\n",
    "\n",
    "            self.vocab = Vocabulary(self.tokenizer)\n",
    "            for i, (count, word) in enumerate(words):\n",
    "                self.vocab.word2index[word] = i\n",
    "                self.vocab.word2count[word] = count\n",
    "                self.vocab.index2word[i] = word\n",
    "                self.vocab.count += 1\n",
    "            \n",
    "            pickle.dump(self.vocab, open(self.vocab_path, 'wb'))\n",
    "        else:\n",
    "            self.vocab = pickle.load(open(self.vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15bf038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextClassificationDataset(train_df, tokenizer, CLASSES, VOCAB_PATH, MAX_LEN, MIN_COUNT)\n",
    "valid_ds = TextClassificationDataset(valid_df, tokenizer, CLASSES, VOCAB_PATH, MAX_LEN, MIN_COUNT)\n",
    "test_ds = TextClassificationDataset(test_df, tokenizer, CLASSES, VOCAB_PATH, MAX_LEN, MIN_COUNT)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f3eb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text shape: torch.Size([400])\n",
      "text shape: ()\n",
      "number of texts: 40000\n",
      "['negative', 'positive']\n",
      "{'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "print('text shape:', train_ds[0][0].shape)\n",
    "print('text shape:', train_ds[0][1].shape)\n",
    "print('number of texts:', len(train_ds))\n",
    "print(train_ds.classes)\n",
    "print(train_ds.class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea01c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> when i put this movie in my dvd player , and sat down with a coke and some chips , i had some expectations . i was hoping that this movie would contain some of the strong points of the first movie awsome animation , good flowing story , excellent voice cast , funny comedy and a kick ass soundtrack . but , to my disappointment , not any of this is to be found in atlantis milo 's return . had i read some reviews first , i might not have been so let down . the following paragraph will be directed to those who have seen the first movie , and who enjoyed it primarily for the points mentioned . when the first scene appears , your in for a shock if you just picked atlantis milo 's return from the display case at your local <unk> or whatever , and had the expectations i had . the music feels as a bad imitation of the first movie , and the voice cast has been replaced by a not so fitting one . with the exception of a few characters , like the voice of sweet . the actual drawings is nt that bad , but the animation in particular is a sad sight . the storyline is also pretty weak , as its more like three episodes of <unk> doo than the single adventurous story we got the last time . but do nt misunderstand , it 's not very good <unk> doo episodes . i did nt laugh a single time , although i might have <unk> once or twice . to the audience who have n't seen the first movie , or do n't especially care for a similar sequel , here is a fast review of this movie as a stand alone product if you liked <unk> doo , you might like this movie . if you did n't , you could still enjoy this movie if you have nothing else to do . and i suspect it might be a good kids movie , but i would n't know . it might have been better if milo 's return had been a three episode series on a cartoon channel , or on breakfast tv .\n"
     ]
    }
   ],
   "source": [
    "# convert back the sequence of integers into original text\n",
    "print(' '.join([train_ds.vocab.index2word[i.item()] for i in train_ds[1][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c724f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I put this movie in my DVD player, and sat down with a coke and some chips, I had some expectations. I was hoping that this movie would contain some of the strong-points of the first movie: Awsome animation, good flowing story, excellent voice cast, funny comedy and a kick-ass soundtrack. But, to my disappointment, not any of this is to be found in Atlantis: Milo's Return. Had I read some reviews first, I might not have been so let down. The following paragraph will be directed to those who have seen the first movie, and who enjoyed it primarily for the points mentioned.<br /><br />When the first scene appears, your in for a shock if you just picked Atlantis: Milo's Return from the display-case at your local videoshop (or whatever), and had the expectations I had. The music feels as a bad imitation of the first movie, and the voice cast has been replaced by a not so fitting one. (With the exception of a few characters, like the voice of Sweet). The actual drawings isnt that bad, but the animation in particular is a sad sight. The storyline is also pretty weak, as its more like three episodes of Schooby-Doo than the single adventurous story we got the last time. But dont misunderstand, it's not very good Schooby-Doo episodes. I didnt laugh a single time, although I might have sniggered once or twice.<br /><br />To the audience who haven't seen the first movie, or don't especially care for a similar sequel, here is a fast review of this movie as a stand-alone product: If you liked schooby-doo, you might like this movie. If you didn't, you could still enjoy this movie if you have nothing else to do. And I suspect it might be a good kids movie, but I wouldn't know. It might have been better if Milo's Return had been a three-episode series on a cartoon channel, or on breakfast TV.\n"
     ]
    }
   ],
   "source": [
    "# print the original text\n",
    "print(train_ds.text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b437636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 26404\n",
      "\n",
      "Most common words:\n",
      "the: 533537\n",
      ",: 435160\n",
      ".: 376449\n",
      "and: 259714\n",
      "a: 257852\n",
      "of: 231500\n",
      "to: 214610\n",
      "is: 173742\n",
      ">: 161704\n",
      "it: 150680\n"
     ]
    }
   ],
   "source": [
    "vocab = train_ds.vocab\n",
    "freqs = [(count, word) for (word, count) in vocab.word2count.items() if count >= MIN_COUNT]\n",
    "vocab_size = len(freqs) + 2  # for PAD and UNK tokens\n",
    "print(f'Vocab size = {vocab_size}')\n",
    "\n",
    "print('\\nMost common words:')\n",
    "for c, w in sorted(freqs, reverse=True)[:10]:\n",
    "    print(f'{w}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cbada",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86de485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention computes a weighted average of the hidden states of the LSTM Model.\n",
    "# In fact, it produce a weight for each hidden state at different time steps\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs = [batch size, sent len, hid dim]\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        # energy = [batch size, sent len, 1]\n",
    "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
    "        # weights = [batch size, sent len]\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        # outputs = [batch size, hid dim]\n",
    "        return outputs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc02667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout= 0 if n_layers < 2 else dropout)\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [sent len, batch size]\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # use 'batch_first' if you want batch size to be the 1st para\n",
    "        # output = [sent len, batch size, hid dim*num directions]\n",
    "        output = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n",
    "        # output = [sent len, batch size, hid dim]\n",
    "        ouput = output.permute(1, 0, 2)\n",
    "        # ouput = [batch size, sent len, hid dim]\n",
    "        new_embed, weights = self.attention(ouput)\n",
    "        # new_embed = [batch size, hid dim]\n",
    "        # weights = [batch size, sent len]\n",
    "        new_embed = self.dropout(new_embed)\n",
    "        return self.fc(new_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15fc05",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6dfe5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, loader, loss_fn, optimizer, metric_fn, device):\n",
    "    \"\"\" training one epoch and calculate loss and metrics \"\"\"\n",
    "    # Training model\n",
    "    model.train()\n",
    "    losses = 0.0\n",
    "    metrics = 0.0\n",
    "    steps = len(loader)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        # Place to gpu\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs.t())\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        losses += loss\n",
    "        # Backpropagation and update weights\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.3)\n",
    "        optimizer.step()\n",
    "        # Calculate metrics\n",
    "        metric = metric_fn(outputs, labels)\n",
    "        metrics += metric\n",
    "        # report\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write('\\r Step: [%2d/%2d], loss: %.4f - acc: %.4f' % (i, steps, loss.item(), metric))\n",
    "    \n",
    "    sys.stdout.write ('\\r')\n",
    "    return losses.item() / len(loader), metrics / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36e5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\" calculate percent of true labels \"\"\"\n",
    "    # predicted labels\n",
    "    _, preds = torch.max(outputs, dim = 1)\n",
    "    return torch.sum(preds == labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4831cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn, metric_fn, device):\n",
    "    \"\"\" Evaluate trained weights using calculate loss and metrics \"\"\"\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    losses = 0.0\n",
    "    metrics = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs.t())\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            losses += loss\n",
    "            metrics += metric_fn(outputs, labels)\n",
    "    return losses.item() / len(loader), metrics / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9333a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, num_epochs, metric_fn, device, checkpoint_path, scheduler=None, load_model=False, prev_best_loss=None):\n",
    "    \"\"\" fiting model to dataloaders, saving best weights and showing results \"\"\"\n",
    "    # to continue training from saved weights\n",
    "    if load_model:\n",
    "        load_checkpoint(torch.load(checkpoint_path), model)\n",
    "    if prev_best_loss is None:\n",
    "        best_loss = 1000000\n",
    "    else:\n",
    "        best_loss = prev_best_loss\n",
    "    losses, val_losses, accs, val_accs = [], [], [], []\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss, acc = train_one(model, train_dl, loss_fn, optimizer, metric_fn, device)\n",
    "        val_loss, val_acc = evaluate (model, valid_dl, loss_fn, metric_fn, device)\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        # learning rate scheduler------\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # save weights if improved-----\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_wts = model.state_dict().copy()\n",
    "            torch.save(best_wts, checkpoint_path)\n",
    "            print('Checkpoint Saved!')\n",
    "        print('Epoch [{}/{}], loss: {:.4f} - acc: {:.4f} - val_loss: {:.4f} - val_acc: {:.4f}'.format (epoch + 1, num_epochs, loss, acc, val_loss, val_acc))\n",
    "\n",
    "    period = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format (period // 60, period % 60))\n",
    "    model.load_state_dict(best_wts)\n",
    "    return dict(loss = losses, val_loss = val_losses, acc = accs, val_acc = val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd1958",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "990f324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 1024\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "# training parameters\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 2\n",
    "CHECKPOINT_PATH = 'IMDB_Normal.pth'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "464a5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionLSTM(vocab_size, EMBED_SIZE, HIDDEN_SIZE, train_ds.num_classes, NUM_LAYERS, bidirectional=True, dropout=0.5).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.7, 0.99))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d7a108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Saved! loss: 0.2415 - acc: 0.9200\n",
      "Epoch [1/2], loss: 0.4855 - acc: 0.7508 - val_loss: 0.2930 - val_acc: 0.8768\n",
      "Checkpoint Saved! loss: 0.0787 - acc: 0.9800\n",
      "Epoch [2/2], loss: 0.2309 - acc: 0.9086 - val_loss: 0.2500 - val_acc: 0.8974\n",
      "Training complete in 433m 17s\n"
     ]
    }
   ],
   "source": [
    "history = train(model, train_dl, valid_dl, loss_fn, optimizer, NUM_EPOCHS, accuracy, DEVICE, CHECKPOINT_PATH, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af98c9",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40516018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bf13725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25064966201782224, 0.8954000000000001)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_dl, loss_fn, accuracy, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
